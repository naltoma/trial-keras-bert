# みんな大好きう・ん・ち・く♡

<hr>

## <a name="singularity">Singularity</a>
- Singularity: [official web](https://github.com/hpcng/singularity), [doc](https://sylabs.io/docs/), [コンテナイメージtips](https://kekcc.kek.jp/service/kekcc/support/ja/container-guide/)
    - 人によって実行環境を変更したい（例えばある人はTensorFlow 1.x を使いたいが、別の人は2.xを使いたいとか）が、それらを無制限に許可すると競合したり環境が壊れたりして不具合の要因となる。これらを防ぐためコンテナの一種であるSingularityを使う。
        - VMという手もあるがリソース的に無駄が大きく管理が手間だし、（計測してないので実際のところはわからないが）VM内モジュールから物理GPUへアクセスだけで遅くなりそう。
    - Singularityでは予め用意したSIFコンテナイメージファイル（SIFファイル）を使うことで個々人の用途にあった環境を利用することができる。PyTorchのような代表的な環境については[dockerhub:pytorch/pytorch](https://hub.docker.com/r/pytorch/pytorch)としてコンテナイメージが用意されており、それを利用する（dockerファイルからsifファイルを生成してる？）ことが可能。ただしSIFファイルは「後から環境構築し直す」ことは許可されておらず、例えば「pip install hoge」でhogeモジュールを追加することはできない。カスタマイズしたいならば最初からSIFファイル（もしくはDockerイメージ）をビルドする必要がある。
    - defファイルで [From: ubuntu:16.04](https://sylabs.io/guides/3.7/user-guide/build_a_container.html#converting-containers-from-one-format-to-another) としてゼロから環境構築することも可能。しかしGPU対応パッケージをビルドするのはそれなりに手間がかかることが多く、不十分なパッケージで構築した場合には結果的にGPUを十分に活かせないどころか「実は全く使わない」ことにもなりかねない。これを避けるため代表的なパッケージをベースにしつつ、それをsandboxとして編集してからコンテナイメージを作成するという流れが恐らくベター。
        - 例えば、、
            - GPU対応TensorFlow (pip): ``singularity build --fakeroot --sandbox test docker://tensorflow/tensorflow:latest-gpu-py3``
            - GPU対応PyTorch (conda): ``singularity build --fakeroot --sandbox test docker://pytorch/pytorch:latest``
            - なお、わざわざpip, conda と書いているのは、原則として[pip, condaをごちゃまぜにして環境構築するのは辞めた方が良い](http://onoz000.hatenablog.com/entry/2018/02/11/142347)から。どちらか片方に統一したほうが良いです。
    - コンテナを起動すると自動的に「singularityを実行した人のホームディレクトリ」がマウントされる。自身のプログラム等は自身のディレクトリ上に置くだけで良く、その実行に必要な環境をSIFファイルとして用意することになる。

<hr>

## <a name="slurm">Slurmって？</a>
- Slurm: [doc](https://slurm.schedmd.com/documentation.html), [ジョブの実行方法](https://www.j-focus.jp/user_guide/ug0004020000/)
    - 共有サーバだが誰もが自由にプログラムを実行すると各プロセスによるリソース競合が発生し、結果的に全体としてのパフォーマンスも落ちてしまう。これらを避けるためプログラム実行をジョブとして設定し、どのジョブをどのリソースで実行するかを管理するツールがジョブ管理システムSlurmである。
    - Slurmで実行するには通常実行とは異なる手順を要する。まずプログラムを実行する際に必要となる情報をバッチファイルとして記述する。このバッチファイルには「標準出力、標準エラー出力、実行するノード数やGPU数、実行するコマンド群」等を必要に応じて記述する。記述を終えたら「slurm バッチファイル名」としてジョブを投入すると、実行可能になった段階で自動的に実行され、実行を終えると指定した標準出力・エラー出力等に結果が記載される。自動でメール送信するなど通知することもカスタマイズ次第で可能（Slurmの機能ではなく、そういうプログラムを実行するように書けば良い）。
- バッチ処理を踏まえた開発の流れ
    - ローカルPCでは「breakpoint指定してデバッグ実行しながら動作確認する」「printデバッグする」「コード修正する都度こまめに実行して動作確認する」ようなことをしながら開発することが多い。これに対してSlurmに投入したジョブはいつ実行されるかはわからず、インタラクティブに実行途中の結果を見るというようなアプローチは困難である。
    - このことを踏まえると基本的には「**小さく動作確認しやすい状況をローカルPCで用意し、そこで開発**」する。機械学習ならばデータセットを小さくするとかモデルを小さくするとか、何かしら小さく動作確認しやすい状況を用意する。その上で、**ある程度動作確認できた後で実際の大規模データセットに対する実行を singularity + slurm でやる**。このように開発フェーズと実行フェーズを分けて考えよう。

<hr>

## BERTって？
- 言語モデルの一種。
    - 言語モデルとは、自然言語で記述された文章を数値的に扱うために、確率分布による表現やベクトル空間へ写像によりモデル化されたものを指す。タスク次第で言語モデルは異なることが一般的である。例えば「誤解」「与える」が共起してる文ならばどこそこの人が喋ったのだなという識別器が作れるだろう。
    - とはいえ、毎回ゼロから言語モデルを作るのは手間暇がかかるし、自動化できるとしても膨大な計算量がかかる。例えば[BERT](https://github.com/google-research/bert)は ``pre-train a BERT-Base on a single preemptible Cloud TPU v2, which takes about 2 weeks at a cost of about $500 USD`` とか、アホらしく時間かかる。これ一回で済むならまだ我慢するかもしれないけど、世の中のデータは変わり続けるし、そもそもチューニングするのが一般的なので一度の実行で最適な言語モデルを構築できる可能性は限りなく低い。やってられない。そんな中でここ数年（10年？）ぐらい盛んに進められているのが「ある程度汎用的なモデルを作り、それを本来やりたかったタスクに合うように追加学習させる」というアプローチ。その切っ掛けの一つを作ったのがBERT。

<hr>

## SentencePieceって？
- [SentencePiece](https://github.com/google/sentencepiece)
- サブワードの一種。
    - 文章を言語モデル化するには必ず「トークン」に分割する必要がある。トークンとは文章を構成する要素のこと。例えば一つ手前の文を mecab で形態素解析させてみると次のようになる。このように分割して考えるのは、「文章は文の集まり」「文は単語や助詞などの集まり」のように考えるほうが実態にも即しているから。これを「トークンの集まりとして文章を処理する」ためにトークン化が必要。

```shell
(base) oct:tnal% mecab
トークンとは文章を構成する要素のこと
トークン	名詞,固有名詞,人名,一般,*,*,トークン,トークン,トークン
と	助詞,格助詞,引用,*,*,*,と,ト,ト
は	助詞,係助詞,*,*,*,*,は,ハ,ワ
文章	名詞,一般,*,*,*,*,文章,ブンショウ,ブンショー
を	助詞,格助詞,一般,*,*,*,を,ヲ,ヲ
構成	名詞,サ変接続,*,*,*,*,構成,コウセイ,コーセイ
する	動詞,自立,*,*,サ変・スル,基本形,する,スル,スル
要素	名詞,一般,*,*,*,*,要素,ヨウソ,ヨーソ
の	助詞,連体化,*,*,*,*,の,ノ,ノ
こと	名詞,非自立,一般,*,*,*,こと,コト,コト
```

- トークン化の問題点。
    - 様々な手法・ツールがあるが、どの場合であっても「単語」に着目してしまうと**未知語が出てきた場合には対応できない**（言語モデルとして表現できない）。
    - また既出単語の組み合わせであったとしても **「適切な区切り方」が一意に定まるとは限らない**。例えば「琉球大学工学部工学科知能情報コース」はどのように区切るのが良いのだろうか、それともこういうユニークな単語として辞書に登録してしまうのが良いのだろうか。他にも問題は考えられるが、とにかく「単語」に着目した時点で負けてる側面がある。（勝ってる部分ももちろん数多くあるので、それらのツールを使う事例は無数にある）
- サブワード！
    - ``単語(Word)ではなく文字(Letter)に着目したら良いんじゃ？`` ということで提案されたのがサブワード。
        - 例えば「where」を 2 lettersの組み合わせで考えると ``_w, wh, he, er, re, e_`` というトークンの組み合わせとして考える。underscoreは実際には冒頭とかスペースとか専用トークンを使うことが多いけども、基本的にはこういう考え方。また、実際にはトークンの出現しやすさ（しにくさ）を情報量として捉え、全トークンを残すのではなく最適なトークン集合を求めて利用することが一般的。
        - 一度トークン集合を用意できれば、「トークン学習時に出現しなかったletter」でない限りは、あらゆる組み合わせの未知語に対応できる。これが最も大きな強み。例えば「ぐぐる」「タピる」みたいな「＊＊る」という用語が生み出されたりするわけだが、これらから学習しておけば「あべる」もそういうニュアンスとして使える見込みがある。
        - 一方で[fastTextのsubword(部分語)の弊害](https://studylog.hateblo.jp/entry/2016/09/20/103724)みたいな話もあって、発展途上とも言える。
- サブワードの一種SentencePieceなら、更にサンプリングによる擬似水増しも！
    - サブワード自体はいろんなツールがあるけれども、今回SentencePieceを使ってる理由はサンプリングもできるから。他のサブワードツールは学習後の最適トークンに分割するだけ。これに対してSentencePieceでは、[README-aug.md](./READEME-aug.md)で例示してるように「同じ文章から異なるトークン分割」を生成できる。（最適分割だけにすることもできる）
    - 1つの文章から異なるトークン分割結果を用意できるし、先にも述べたとおり「適切な区切り方」自体がそもそも明確でないならば、確率的に尤もらしい区切り方を複数用意することで多様な区切りを踏まえた学習をしてくれる可能性がでてくる。一種の正則化として機能するかもしれない（多分。めいびー）。
- データ水増し？
    - 画像なら例えばグレイスケールにする、拡大縮小する、角度を変える、ノイズ付与するとかあれこれある。これの自然言語版として「ある単語を上位関係にある単語に置き換える」とか「文節単位で入れ替える」とかあるのだけれども、画像と比べるとスマートじゃない方法が結構多い。ここでいうスマートじゃないというのは、単純なルールでは文が崩れてしまう可能性があるという意味。
    - これに対し前述のSentencePieceでのトークン分割という視点での水増しは、文そのものは全く同じなので崩れる可能性はありえない。

<hr>

## トークンIDで学習
文章をトークン化したあとの処理は、トークンのままだと扱いづらいので実際にはトークンIDで処理することが多いです。サンプリング結果をトークンIDで出力してみると以下のような感じになります。ID列から見るとこれが同じ文章だとは思えませんね。でも、機械学習におけるモデルはこういう数値列をみて学習していることが多く、結果的に「理解しているとしか思えない動作」をしています。機械学習って何を学んでいるんでしょうね？

```python
>>> for i in range(10):
...     print(sp.encode(sentence, out_type=int, enable_sampling=True, nbest_size=-1, alpha=0.01))
... 
[444, 333, 60, 103, 19, 263, 8]
[9, 903, 11, 2666, 19, 263, 8]
[9, 315, 964, 11, 2666, 2767, 8]
[2007, 11, 6295, 103, 19, 263, 8]
[2007, 11, 333, 60, 103, 19, 263, 8]
[9, 903, 11, 333, 526, 2767, 8]
[9, 903, 11, 333, 60, 103, 2767, 8]
[2007, 11, 333, 526, 19, 263, 8]
[444, 6295, 103, 19, 263, 8]
[9, 315, 964, 11, 333, 526, 19, 263, 8]
```
